<!doctype html><html lang=en><head><title>An AI/ML accuracy tale · Jens Rantil
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Jens Rantil"><meta name=description content="Estimating accuracy for AI/ML systems isn't as simple as one would think."><meta name=keywords content="blog,developer,staff engineering,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="An AI/ML accuracy tale"><meta name=twitter:description content="Estimating accuracy for AI/ML systems isn't as simple as one would think."><meta property="og:url" content="https://jensrantil.github.io/posts/ai-ml-accuracy-tale/"><meta property="og:site_name" content="Jens Rantil"><meta property="og:title" content="An AI/ML accuracy tale"><meta property="og:description" content="Estimating accuracy for AI/ML systems isn't as simple as one would think."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-18T23:55:35+02:00"><meta property="article:modified_time" content="2024-02-18T12:26:32+01:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="ML"><meta property="article:tag" content="Service Levels"><link rel=canonical href=https://jensrantil.github.io/posts/ai-ml-accuracy-tale/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.7763f8bc6341ecf82378e867c285e1549abb063a899be313ccd25dbfcd24fa7d.css integrity="sha256-d2P4vGNB7PgjeOhnwoXhVJq7BjqJm+MTzNJdv80k+n0=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/custom.min.2e198fefc57dcc517a51f5b2040b841f014759bd551d4dd18df6dd0c1b954a62.css integrity="sha256-LhmP78V9zFF6UfWyBAuEHwFHWb1VHU3RjfbdDBuVSmI=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://jensrantil.github.io/>Jens Rantil
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/pages/services/>Services</a></li><li class=navigation-item><a class=navigation-link href=/pages/about-me/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://jensrantil.github.io/posts/ai-ml-accuracy-tale/>An AI/ML accuracy tale</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2023-11-18T23:55:35+02:00>November 18, 2023
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
5-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/ai/>AI</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/ml/>ML</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/service-levels/>Service Levels</a></span></div></div></header><div class=post-content><p>I recently read the article <a href=https://shreyans.org/google class=external-link target=_blank rel=noopener>&ldquo;What I learned getting acquired by
Google&rdquo;</a> by Shreyans Bhansali. Shreyans wrote</p><blockquote><p>On the other hand there was the discovery that most Search improvements are
manually reviewed by engineers through ‘side-by-side’ comparisons between old
and new results&mldr;on spreadsheets!</p></blockquote><p>The above quote reminded of how hard, and often understated, quality assurance
(QA) in AI/ML systems is. Each change to a model needs to be validated, and
validation is <em>hard</em> and <em>cumbersome</em>. Also, the fact that models can have a
<em>freshness</em> does not help - that means that quality assurance must be done
continuously and treated as a <a href=https://sre.google/sre-book/service-level-objectives/ class=external-link target=_blank rel=noopener>service level</a>.</p><p>To make my case, I thought I could share a tale about a classification system I
used to work on a bit.</p><h2 id=a-tale>A tale
<a class=heading-link href=#a-tale><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>At a former employer we had a system that categorized a stream of financial
transactions using ML. For example, &ldquo;MacDonald&rsquo;s&rdquo; was categorized as
&ldquo;Restaurant&rdquo;, and &ldquo;H&amp;M&rdquo; was categorized as &ldquo;Clothing&rdquo;, etc. If we were
uncertain, we set the category &ldquo;Uncategorized&rdquo;. The users could adjust
incorrectly categorized transactions if they were wrong. Our goal was to
measure the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision class=external-link target=_blank rel=noopener>accuracy</a> of how well these categories were applied by
the ML model.</p><blockquote><p>Was this category (in)correct?</p></blockquote><p>Initially, we considered to ask for explicit feedback (&ldquo;Was this category
correct?&rdquo;) from the user in the UI. However, we concluded that we did not want
to make our UX bloated. We asked ourselves, can we somehow figure out whether
our classification is accurate without changing our UX?</p><p>Our first iteration was a service level based on the ratio between the “number
of manual corrections” and “total categorizations”. This did not work very well
for two big reasons: Partially because it varied immensely between users and
how eager they were to adjust incorrectly categorized data or not. But mostly
because a lot of users were only adjusting the categories when they had bought
something different in a store; ie. buying &ldquo;makeup&rdquo; from &ldquo;H&amp;M&rdquo; instead of
&ldquo;Clothing&rdquo;. This made our numbers look much worse than they were! We did not
get any positive feedback on the correct classifications.</p><p>Our next take was to not categorize 1% of all financial transactions to force
our users to set the correct category. When they did set it, we compared what
they set against what our model would have guessed. Our service level was
defined as the &ldquo;number of adjustments that matched our ML model&rsquo;s guess&rdquo;
divided by the &ldquo;total number of adjustments&rdquo;.</p><p>Randomly not categorising 1% of all financial transactions was a good idea! But
it turned out to have a surprising backlash from users; They perceived our
classification as accuracy having become significantly worse:</p><blockquote><p>&ldquo;Why are you unable to categorize &lsquo;McDonalds&rsquo;?? C&rsquo;mon, I expect better from
your product!</p></blockquote><p>It turns out, we were not classifying some of the things we were certain about.
Could we do better?</p><p>We were lucky that our ML could spit out a certainty measure for our
classification between [0,1]. We started using the probability of skipping
categorizing a transaction based on the inverse of that certainty. That meant
&ldquo;McDonald&rsquo;s&rdquo;, having a high certainty, was rarely skipped anymore. Good!</p><p>Instead of certainty, the inverse frequency could have been a different factor
to use to avoid common descriptions being randomly skipped. As far as I know,
we never pursued that approach.</p><p>Through a series of events, our business pivoted and started having customers
using our classification API and presenting the result in a UI of their own.
Since the customers could not always adjust incorrect categories in the UI, we
had to resort to manual quality assurance instead where people would sit and
verify that categories were correctly identified. This also made it hard for us
to treat ML accuracy as a service level. We instead had to do more quality
assurance before release time. Unfortunately that lead to longer iteration
cycles for new models, but at least we knew we could trust the data fairly
well.</p><h2 id=on-the-topic-of-unit-tests>On the topic of unit tests
<a class=heading-link href=#on-the-topic-of-unit-tests><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We <em>did</em> have unit tests in place for the descriptions of common financial
transactions. And we were, over time, building up a suite of common
descriptions and what they were supposed to match. However, there were a few
problems with this:</p><ul><li>The freshness I mentioned at the beginning of this post was a problem; Unit
tests grew old. &ldquo;Grand Daddy&rsquo;s house&rdquo; was once a pizza in one part of Sweden
and a clothing store in another part of Sweden a couple of years later. We
had to update our unit tests.</li><li>We ended up having different models for different countries/regions. Having
unit tests for all of them didn&rsquo;t scale. Plus, we did not know common
merchants in all the regions - our company was mostly based out of Sweden.</li><li>The whole idea with the ML model was that it was supposed to scale and us
not doing all the manual classification!</li></ul><h2 id=in-conclusion>In conclusion
<a class=heading-link href=#in-conclusion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Measuring AI/ML model quality takes a lot of creativity bordering SRE practises
(such as service levels, release strategies) and UX aspects - and there can be
some fun surprises along the way. :) Sometimes, manual QA through something
like <a href=https://www.mturk.com/ class=external-link target=_blank rel=noopener>Amazon&rsquo;s Mechanical Turk</a> is the easiest way to go about it, but if
you can somehow build in feedback mechanisms through your UX that is usually
much better to continuously measure service quality.</p><p>The customer is always right. Manual quality assurance will never be as
accurate as the one from customers, but it might be good enough.</p></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2026
Jens Rantil
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.
[<a href=https://github.com/JensRantil/jensrantil.github.io/tree/0468926a877b9381c5fb17c32967e8e34e44f58c target=_blank rel=noopener>0468926</a>]</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8QE7N3YHJ3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8QE7N3YHJ3")</script></body></html>